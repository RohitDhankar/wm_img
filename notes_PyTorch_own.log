

-- The second core thing that PyTorch provides is the
ability of tensors to keep track of the operations performed on them and to analyti-
cally compute derivatives of an output of a computation with respect to any of its
inputs. This is used for numerical optimization, and it is provided natively by tensors
by virtue of dispatching through PyTorch‚Äôs autograd engine under the hood.
























#### FOOBAR_Old_PySpark_UnitTests

### https://towardsdatascience.com/stop-mocking-me-unit-tests-in-pyspark-using-pythons-mock-library-a4b5cd019d7e

https://mungingdata.com/pyspark/testing-pytest-chispa/

Towards Data Science

Sign in
Get started
Follow
¬∑
Editors' Picks
Features
Deep Dives
Grow
Contribute
About

Stop mocking me! Unit tests in PySpark using Python‚Äôs mock library
Serena McDonnell

Serena McDonnell

Nov 11, 2018¬∑5 min read

Testing.

Fundamental in software development, and often overlooked by data scientists, but important. In this post, I‚Äôll show how to do unit testing in PySpark using Python‚Äôs unittest.mock library. I‚Äôll do this from a data scientist‚Äôs perspective- to me that means that I won‚Äôt go into the software engineering details. I present just what you need to know.

First, a (semi) relevant clip from Family Guy:
What is a unit test? What‚Äôs a mock?

A unit test is a way to test pieces of code to make sure things work as they should. The unittest.mock library in Python allows you to replace parts of your code with mock objects and make assertions about how they‚Äôve been used. A ‚Äúmock‚Äù is an object that does as the name says- it mocks the attributes of the objects/variables in your code.
The end goal: testing spark.sql(query)

A simple way to create a dataframe in PySpark is to do the following:

df = spark.sql("SELECT * FROM table")

Although it‚Äôs simple, it should be tested.
The code and problem set up

Let‚Äôs say we work for an e-commerce clothing company, and our goal is to create a product similarity table that has been filtered on some conditions, and write it to HDFS.

Assume we have the following tables:

1. Products. Columns: ‚Äúitem_id‚Äù, ‚Äúcategory_id‚Äù.2. Product_similarity (unfiltered). Columns: ‚Äúitem_id_1‚Äù, ‚Äúitem_id_2‚Äù, ‚Äúsimilarity_score‚Äù.

(Assume the score in product_similarity is between 0 and 1, where the closer the score is to 1, the more similar the items. See my post on similarity metrics for more details if you‚Äôre interested).

Looking at pairs of items and their scores is as simple as:

The where clause is to remove rows comparing an item to itself. The score will always be 1. How boring!

But what if we want to create a table that shows us similarity of items that are in the same category? What if we don‚Äôt care to compare shoes to scarves, but we want to compare shoes to shoes and scarves to scarves? This is a bit more complicated, and requires us to join the ‚Äúproducts‚Äù and ‚Äúproduct_similarity‚Äù tables.

The query would then become:

We may also want to get the most N similar items for each product, so in that case, our query would become:

(Assuming we use N = 10).

Now, what if we want the option to either compare products across categories, or only within categories? We could achieve this by using a boolean variable called same_category that results in a string same_category_q that can be passed into the overall query (using .format()), and will be equal to the inner join above if our boolean same_category is True, and empty if it‚Äôs False. The query would then look like:

Let‚Äôs make it a bit more clear, and wrap this logic in a function that returns same_category_q:

So far, so good. We output the query same_category_q so we can test our function to make sure it returns what we want it to return.

Keeping in mind our final goal, we want to write a dataframe to HDFS. We can do this with the following function:

Adding the first part of the query and a main method to complete our script, we get:

The idea here is that we want to create a function, generically named test_name_of_function(), for each function in our script. We want to test that the function behaves as it should, and we ensure this by using assert statements all over the place.
Test_make_query, true and false

First, let‚Äôs test the make_query function. Recall that make_query takes in two inputs: a boolean variable and some table paths. It‚Äôll return different values for same_category_q based on the boolean same_category. What we‚Äôre sort of doing here is like a set of if-then statements:

1. If same_category is True, then same_category_q = ‚ÄúINNER JOIN ‚Ä¶‚Äù2. If same_category is False, then same_category_q = ‚Äú‚Äù (empty)

What we do is mock make_query‚Äòs parameters and pass them in, then test that we got our desired outputs. Since test_paths is a dictionary, we don‚Äôt need to mock it. The test script is below, with comments added for extra clarity.

And it‚Äôs as simple as that!
Testing our table creation

Next up, we need to test that our create_new_table function behaves as it should. Stepping through the function, we see that it does several things, with several opportunities to make some assertions and mocks. Note that whenever we have something like df.write.save.something.anotherthing, we need to mock each operation and its output.

    The function takes spark as a parameter. This needs to be mocked.
    Create created_table by calling spark.sql(create_table_query.format(**some_args)). We want to assert that spark.sql() is called only once. We‚Äôll need to mock the output of spark.sql()as well.
    Coalesce created_table. Ensure that coalesce() is called with the parameter 1. Mock the output.
    Write the coalesced table. We need to mock .write, and mock the output of calling it on our table.
    Save the coalesced table to a save path. Ensure that it‚Äôs been called with the correct parameters.

As before, the test script is below, with comments for clarity.

Finally, save everything in a folder together. You could import the functions from their respective modules if you want to, or just have everything in one script.

To test it, navigate in the command line to your folder (cd folder/working_folder) and call:

python -m pytest final_test.py.

You should see something like (excuse the formatting):

serena@Comp-205:~/workspace$ python -m pytest testing_tutorial.py ============================= test session starts ============================== platform linux -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 rootdir: /home/serena/workspace/Personal, inifile: plugins: mock-1.10.0 collected 3 items testing_tutorial.py ... [100%] =========================== 3 passed in 0.01 seconds ===========================

And that‚Äôs it!

There you have it. I hope this was somewhat helpful. When I was trying to figure out how to mock, I wish I had come across some sort of tutorial like this. Now go ahead, and as Stewie said, (don‚Äôt) stop mocking me (functions)!

Originally published at serena.mcdonnell.ca on November 7, 2018.
Serena McDonnell

Always curious about math. Senior Data Scientist @ Delphia - views are my own. Check out my personal website: serena.mcdonnell.ca.
Follow

Related
Selecting Multiple Columns in PySpark
Discussing how to select multiple columns from PySpark DataFrames by column name, index or with the use of regular expressions
4 Full Practice Tests To Prepare Databricks Associate Certification (PySpark | 2021)
Optimizing Apache Spark 3.0 via PySpark with Caching
Explanation of PySpark and Coding
Apache Spark:
Sign up for The Variable
By Towards Data Science

Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.

    Pyspark
    Mock
    Unit Testing
    Data Science

More from Towards Data Science
Follow

Your home for data science. A Medium publication sharing concepts, ideas and codes.
Read more from Towards Data Science
More From Medium
India‚Äôs Lockdown & Current Scenario Analysis‚Äî Part 2
Srinidhi Kunigal Venkatesha Murthy
January 6 Insurrectionists Face a Variety of Charges: A Closer Look at the Capitol Breach Cases
Austin Kocher, PhD
Malaria Detection using Deep Learning:
Vishnu Dutt Pathak
How to make your Pandas operation 100x faster
Yifei Huang in Towards Data Science
Interactive Network Visualization
Himanshu Sharma in Towards Data Science
Programming, Data Science and Machine Learning Books (Python and R)‚Ää‚Äî‚ÄäOne Zero Blog
Rahul Raoniar in The Researchers‚Äô Guide
Creating Visual Crowds and Machines
Jae Duk Seo
Data Lineage Explained To My Grandmother
Xavier de Boisredon in Towards Data Science

About

Write

Help

Legal

########### CHISPA 


Skip to content
MungingData
Piles of precious data

    HomePySparkTesting PySpark Code

Testing PySpark Code
mrpowers June 13, 2020 1	

This blog post explains how to test PySpark code with the chispa helper library.

Writing fast PySpark tests that provide your codebase with adequate coverage is surprisingly easy when you follow some simple design patters. chispa outputs readable error messages to facilitate your development workflow.

A robust test suite makes it easy for you to add new features and refactor your codebase. It also provides other developers with ‚Äúliving code documentation‚Äù ‚Äì they can see the inputs and outputs of your functions.
Example project

The pysparktestingexample project was created with Poetry, the best package manager for PySpark projects. All the code covered in this post is in the pysparktestingexample repo.

Here are the commands that were run to setup the project:

    poetry new pysparktestingexample: creates a new project and automatically includes pytest
    poetry add pyspark: adds PySpark to the project
    poetry add chispa --dev: adds chispa as a development dependency

chispa is only needed in the test suite and that‚Äôs why it‚Äôs added as a development dependency.

Your pypoetry.toml file will look like this after running the commands.

    [tool.poetry]
    name = "pysparktestingexample"
    version = "0.1.0"
    description = ""
    authors = ["MrPowers <matthewkevinpowers@gmail.com>"]
    [tool.poetry.dependencies]
    python = "^3.7"
    pyspark = "^2.4.6"
    [tool.poetry.dev-dependencies]
    pytest = "^5.2"
    chispa = "^0.3.0"
    [build-system]
    requires = ["poetry>=0.12"]
    build-backend = "poetry.masonry.api"

Poetry sets up a virtual environment with the PySpark, pytest, and chispa code that‚Äôs needed for this example application.
SparkSession

Let‚Äôs start by setting up the SparkSession in a pytest fixture, so it‚Äôs easily accessible by all our tests.

You‚Äôll use the SparkSession frequently in your test suite to build DataFrames.

Create a tests/conftest.py file and add this code:

    import pytest
    from pyspark.sql import SparkSession
    @pytest.fixture(scope='session')
    def spark():
        return SparkSession.builder \
          .master("local") \
          .appName("chispa") \
          .getOrCreate()

Let‚Äôs use this fixture to create a test that compares the equality of two columns.
Column equality

Create a functions.py file and add a remove_non_word_characters function that‚Äôll remove all the non-word characters from a string.

    import pyspark.sql.functions as F
    def remove_non_word_characters(col):
        return F.regexp_replace(col, "[^\\w\\s]+", "")

Let‚Äôs write a test that makes sure this function removes all the non-word characters in strings.

We‚Äôll put this code in a tests/test_functions.py file.

Create a DataFrame with a column that contains non-word characters, run the remove_non_word_characters function, and check that all the non-word characters are removed with the chispa assert_column_equality method.

    import pytest
    from pysparktestingexample.functions import remove_non_word_characters
    from chispa import *
    import pyspark.sql.functions as F
    def test_remove_non_word_characters(spark):
        data = [
            ("jo&&se", "jose"),
            ("**li**", "li"),
            ("#::luisa", "luisa"),
            (None, None)
        ]
        df = spark.createDataFrame(data, ["name", "expected_name"])\
            .withColumn("clean_name", remove_non_word_characters(F.col("name")))
        assert_column_equality(df, "clean_name", "expected_name")

The DataFrame is initally created with the ‚Äúinput‚Äù and ‚Äúexpected‚Äù values. withColumn appends the ‚Äúactual‚Äù value that‚Äôs returned from running the function that‚Äôs being tested.

Tests generally compare ‚Äúactual‚Äù values with ‚Äúexpected‚Äù values. In this example, clean_name (actual value) is compared with expected_name (expected value).

Notice that one row has an input value equal to None. Always test the None case and make sure the code does not error out.

Notice that def test_remove_non_word_characters(spark) includes a reference to the ‚Äúspark‚Äù fixture we created in the conftest.py file. Remember that this fixture is what provides you with access to the SparkSession in your tests so you can create DataFrames.
Failing column equality test

Let‚Äôs write another test that‚Äôll error out and inspect the test output to see how to debug the issue.

Here‚Äôs the failing test.

    def test_remove_non_word_characters_nice_error(spark):
        data = [
            ("matt7", "matt"),
            ("bill&", "bill"),
            ("isabela*", "isabela"),
            (None, None)
        ]
        df = spark.createDataFrame(data, ["name", "expected_name"])\
            .withColumn("clean_name", remove_non_word_characters(F.col("name")))
        assert_column_equality(df, "clean_name", "expected_name")

This‚Äôll return a nicely formatted error message:

We can see the matt7 / matt row of data is what‚Äôs causing the error because it‚Äôs colored red. The other rows are colored blue because they‚Äôre equal.

Descriptive error messages are an advantage of the chispa library. Other libraries don‚Äôt output error messages that allow developers to easily identify mismatched rows.
DataFrame equality

Create a transformations.py file and add a sort_columns method that sorts the columns of a DataFrame in ascending or descending alphabetical order.

    def sort_columns(df, sort_order):
        sorted_col_names = None
        if sort_order == "asc":
            sorted_col_names = sorted(df.columns)
        elif sort_order == "desc":
            sorted_col_names = sorted(df.columns, reverse=True)
        else:
            raise ValueError("['asc', 'desc'] are the only valid sort orders and you entered a sort order of '{sort_order}'".format(
                sort_order=sort_order
            ))
        return df.select(*sorted_col_names)

Write a test that creates a DataFrame, reorders the columns with the sort_columns method, and confirms that the expected column order is the same as what‚Äôs actually returned by the function. This test will compare the equality of two entire DataFrames. It‚Äôll be different than the previous test that compared the equality of two columns in a single DataFrame.

Here‚Äôs the test that‚Äôll be added to the tests/test_transformations.py file.

    from pysparktestingexample.transformations import sort_columns
    from chispa.dataframe_comparer import assert_df_equality
    import pyspark.sql.functions as F
    def test_sort_columns_asc(spark):
        source_data = [
            ("jose", "oak", "switch"),
            ("li", "redwood", "xbox"),
            ("luisa", "maple", "ps4"),
        ]
        source_df = spark.createDataFrame(source_data, ["name", "tree", "gaming_system"])
        actual_df = sort_columns(source_df, "asc")
        expected_data = [
            ("switch", "jose", "oak"),
            ("xbox", "li", "redwood"),
            ("ps4", "luisa", "maple"),
        ]
        expected_df = spark.createDataFrame(expected_data, ["gaming_system", "name", "tree"])
        assert_df_equality(actual_df, expected_df)

This test is run with the assert_df_equality function defined in chispa.dataframe_comparer. The assert_column_equality method isn‚Äôt appropriate for this test because we‚Äôre comparing the order of multiple columns and the schema matters. Use the assert_column_equality method whenever possible and only revert to assert_df_equality when necessary.

Let‚Äôs write another test to verify that the sort_columns method can also rearrange the columns in descending order.

    def test_sort_columns_desc(spark):
        source_data = [
            ("jose", "oak", "switch"),
            ("li", "redwood", "xbox"),
            ("luisa", "maple", "ps4"),
        ]
        source_df = spark.createDataFrame(source_data, ["name", "tree", "gaming_system"])
        actual_df = sort_columns(source_df, "desc")
        expected_data = [
            ("oak", "jose", "switch"),
            ("redwood", "li", "xbox"),
            ("maple", "luisa", "ps4"),
        ]
        expected_df = spark.createDataFrame(expected_data, ["tree", "name", "gaming_system"])
        assert_df_equality(actual_df, expected_df)

In a real codebase, you‚Äôd also want to write a third test to verify that sort_columns throws an error when the second argument is an invalid value (asc and desc are the only valid values for the second argument). We‚Äôll skip that test for now, but it‚Äôs important for your test suite to verify your code throws descriptive error messages.
Failing DataFrame equality test

Let‚Äôs write a DataFrame comparison test that‚Äôll return an error.

Create a modify_column_names function in the transformations.py file that‚Äôll update all the column names in a DataFrame.

    def modify_column_names(df, fun):
        for col_name in df.columns:
            df = df.withColumnRenamed(col_name, fun(col_name))
        return df

Now create a string_helpers.py file with a dots_to_underscores method that converts the dots in a string to underscores.

    def dots_to_underscores(s):
        return s.replace(".", "_", 1)

Write a test to verify that modify_column_names converts all the dots are converted to underscores.

    import pysparktestingexample.transformations as T
    import pysparktestingexample.string_helpers as SH
    def test_modify_column_names_error(spark):
        source_data = [
            ("jose", 8),
            ("li", 23),
            ("luisa", 48),
        ]
        source_df = spark.createDataFrame(source_data, ["first.name", "person.favorite.number"])
        actual_df = T.modify_column_names(source_df, SH.dots_to_underscores)
        expected_data = [
            ("jose", 8),
            ("li", 23),
            ("luisa", 48),
        ]
        expected_df = spark.createDataFrame(expected_data, ["first_name", "person_favorite_number"])
        assert_df_equality(actual_df, expected_df)

This‚Äôll return a nicely formatted error message:

Our code has a bug. The person.favorite.number column is converted to person_favorite.number and we want it to be converted to person_favorite_number. Your test suite will help you avoid releasing buggy code like this in production üòâ
Approximate column equality

We can check if columns are approximately equal, which is especially useful for floating number comparisons.

Create a divide_by_three function in functions.py that divides a number by three.

    def divide_by_three(col):
        return col / 3

Here‚Äôs a test that uses the assert_approx_column_equality function to compare the equality of two floating point columns.

    def test_divide_by_three(spark):
        data = [
            (1, 0.33),
            (2, 0.66),
            (3, 1.0),
            (None, None)
        ]
        df = spark.createDataFrame(data, ["num", "expected"])\
            .withColumn("num_divided_by_three", divide_by_three(F.col("num")))
        assert_approx_column_equality(df, "num_divided_by_three", "expected", 0.01)

The precision is set to 0.01 in this example. 0.33333333 and 0.33 are considered approximately equal because the absolute value of the difference between the two numbers is less than the specified precision.

Let‚Äôs add another test that‚Äôs failing and inspect the error message.

Here‚Äôs a test that‚Äôll error out because of a row that‚Äôs not approximately equal.

    def test_divide_by_three_error(spark):
        data = [
            (5, 1.66),
            (6, 2.0),
            (7, 4.33),
            (None, None)
        ]
        df = spark.createDataFrame(data, ["num", "expected"])\
            .withColumn("num_divided_by_three", divide_by_three(F.col("num")))
        assert_approx_column_equality(df, "num_divided_by_three", "expected", 0.01)

The error message makes it clear that for one row of data, we‚Äôre expecting num_divided_by_three to equal 4.33, but it‚Äôs actually 2.3333333333333335. Those numbers aren‚Äôt approximately equal when the precision factor is 0.01.
Approximate DataFrame equality

Let‚Äôs create two DataFrames and confirm they‚Äôre approximately equal.

    def test_approx_df_equality_same():
        data1 = [
            (1.1, "a"),
            (2.2, "b"),
            (3.3, "c"),
            (None, None)
        ]
        df1 = spark.createDataFrame(data1, ["num", "letter"])
        data2 = [
            (1.05, "a"),
            (2.13, "b"),
            (3.3, "c"),
            (None, None)
        ]
        df2 = spark.createDataFrame(data2, ["num", "letter"])
        assert_approx_df_equality(df1, df2, 0.1)

The assert_approx_df_equality method is smart and will only perform approximate equality operations for floating point numbers in DataFrames. It‚Äôll perform regular equality for strings and other types.

Let‚Äôs perform an approximate equality comparison for two DataFrames that are not equal.

    def test_approx_df_equality_different():
        data1 = [
            (1.1, "a"),
            (2.2, "b"),
            (3.3, "c"),
            (None, None)
        ]
        df1 = spark.createDataFrame(data1, ["num", "letter"])
        data2 = [
            (1.1, "a"),
            (5.0, "b"),
            (3.3, "z"),
            (None, None)
        ]
        df2 = spark.createDataFrame(data2, ["num", "letter"])
        assert_approx_df_equality(df1, df2, 0.1)

Here‚Äôs the pretty error message that‚Äôs outputted:

Schema mismatch messages

DataFrame equality messages perform schema comparisons before analyzing the actual content of the DataFrames. DataFrames that don‚Äôt have the same schemas should error out as fast as possible.

Let‚Äôs compare a DataFrame that has a string column an integer column with a DataFrame that has two integer columns to observe the schema mismatch message.

    def test_schema_mismatch_message():
        data1 = [
            (1, "a"),
            (2, "b"),
            (3, "c"),
            (None, None)
        ]
        df1 = spark.createDataFrame(data1, ["num", "letter"])
        data2 = [
            (1, 6),
            (2, 7),
            (3, 8),
            (None, None)
        ]
        df2 = spark.createDataFrame(data2, ["num", "num2"])
        assert_df_equality(df1, df2)

Here‚Äôs the error message:
Benefits of testing

A well written test suite makes your code easier to refactor. You be assured that newly added features don‚Äôt break existing logic. Data tends to be messy and there are often lots of edge cases. Your test suite will make sure all the different types of dirty data are handled properly.

The test suite also documents code functionality. Suppose you‚Äôre working with a column that has 5 different types of dirty data that‚Äôs cleaned by a function. You may have a function that cleans this field. Your test suite will provide representative examples of the different types of dirty data and how they‚Äôre standardized. Someone new to the project can read the tests and understand the different types of dirty data that needs to be accounted for without even querying the data in production.

Documenting dirty data attributes in a wiki or in comments is dangerous because the code can change and developers might forget to update the documentation. A test suite serves as ‚Äúliving code documentation‚Äù. Developers should always keep the test suite passing. Whenever you push code to the master branch, you should have a continuous integration server that runs the test suite. If a test fails, fixing it should be a top priority. A test suite, living code documentaion, should never get outdated like traditional documentation.

Functions with side effects or that perform multiple operations are hard to test. It‚Äôs easier to break up code into single purpose modular functions, so they‚Äôre easier to test. Testing encourages developers to write higher quality code.
Next steps

If you can, transition your project to Poetry, as described in this blog post. It‚Äôll make it easier to add development dependencies like pytest and chispa to your project.

You‚Äôll want to leverage dependency injection and mocking to build a great test suite. You‚Äôll also want to wire up your project with continuous integration and continuous deployment.

You‚Äôll find your code a lot easier to reason with when it‚Äôs nice and tested üòâ
Registration
Posted in PySpark	
1 Comment

    Curtis
    1 year ago Permalink

    Hey Matthew, thank you for this article. Testing doesn‚Äôt seem to be talked about much in the data industry, what would you say is good indicators of when you haven‚Äôt tested a pipeline enough and indicators that you have tested too much?

Comments are closed, but trackbacks and pingbacks are open.
Previous Post: PySpark Dependency Management and Wheel Packaging with Poetry
Next Post: Writing out single files with Spark (CSV or Parquet)
Primary Sidebar
Search for:
Recent Posts

    Ultra-cheap international real estate markets in 2022
    Read multiple CSVs into pandas DataFrame
    Scale big data pandas workflows with Dask
    Writing NumPy Array to Text Files
    Content creators making more than $50,000 a month

Recent Comments

    Chris Winne on Chaining Custom PySpark DataFrame Transformations
    KAYSWELL on Serializing and Deserializing Scala Case Classes with JSON
    mrpowers on Exploring DataFrames with summary and describe
    carlo sancassani on Calculating Week Start and Week End Dates with Spark
    Andrew on Exploring DataFrames with summary and describe

Archives

    January 2022
    December 2021
    November 2021
    October 2021
    August 2021
    June 2021
    May 2021
    April 2021
    March 2021
    February 2021
    January 2021
    December 2020
    November 2020
    October 2020
    September 2020
    August 2020
    July 2020
    June 2020
    April 2020
    March 2020
    January 2020
    December 2019
    November 2019
    October 2019
    September 2019
    August 2019
    July 2019
    April 2019
    March 2019
    February 2019
    January 2019
    December 2018
    October 2018
    September 2018
    July 2018
    May 2018
    April 2018
    October 2017
    January 2017

Categories

    Apache Spark
    AWS
    books
    Career
    Creator
    Dask
    Delta Lake
    Emacs
    github
    Golang
    Investing
    Java
    NumPy
    OSS
    Pandas
    PyArrow
    PySpark
    Python
    Scala
    Spark 3
    sqlite
    Unix

Meta

    Log in
    Entries feed
    Comments feed
    WordPress.org

Copyright ¬© 2022 MungingData. Powered by WordPress and Stargazer.
