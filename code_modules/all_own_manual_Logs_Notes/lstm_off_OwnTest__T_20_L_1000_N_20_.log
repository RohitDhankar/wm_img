----initial_data___
 <class 'numpy.ndarray'>
---input.shape----
 torch.Size([17, 999])
---target.shape----
 torch.Size([17, 999])
---test_input.shape----
 torch.Size([3, 999])
---test_target.shape----
 torch.Size([3, 999])
STEP:  0
loss: 0.5018563376414591
loss: 0.4979951386166691
loss: 0.47884127073705207
loss: 0.4474290656285738
loss: 0.34530947622431307
loss: 0.20443325179868263
loss: 3.4661123144031456
loss: 0.062464112442217126
loss: 0.03288893511999246
loss: 0.02846398409071709
loss: 0.027489645662453113
loss: 0.022843181197014152
loss: 0.018014180496762976
loss: 0.010117866962174784
loss: 0.007092074988031853
loss: 0.005163227425355968
loss: 0.0027247856631654977
loss: 0.0016492523969107928
loss: 0.0014604927117199695
loss: 0.0012848247675649389
test loss: 0.0010530041508027936
-------PRED__y_pred.detach().numpy()__ [[-0.26566978 -0.17727469 -0.14309259 ... -0.66837703 -0.66837703
  -0.66837703]
 [-0.59441409 -0.68854763 -0.64217454 ... -0.66837703 -0.66837703
  -0.66837703]
 [-0.49988084 -0.58574541 -0.61393308 ... -0.66837703 -0.66837703
  -0.66837703]]
STEP:  1
loss: 0.0011772055870037808
loss: 0.0010492526272206052
loss: 0.0008305344750233819
loss: 0.0007736142622540367
loss: 0.0007410376833544934
loss: 0.0005926182885927192
loss: 0.0004760015612475358
loss: 0.0003866847776730769
loss: 0.0003501928237959037
loss: 0.0003342609807849309
loss: 0.00032727263495190294
loss: 0.0003249272240711494
loss: 0.0003224704688609707
loss: 0.00031818962647830546
loss: 0.0003113552842608202
loss: 0.00029918499246843765
loss: 0.0002752537442486252
loss: 0.00024641305460566516
loss: 0.0002313163742368194
loss: 0.0002238866640049042
test loss: 6.605567406427307e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.04200905  0.14031224  0.08362755 ... -0.52632831 -0.52632831
  -0.52632831]
 [-0.41676552 -0.47113683 -0.40152193 ...  0.58770964  0.58770964
   0.58770964]
 [-0.28102664 -0.35148843 -0.41033392 ... -0.52632831 -0.52632831
  -0.52632831]]
STEP:  2
loss: 0.00022179876832416592
loss: 0.00022086447273135206
loss: 0.00022041837109435577
loss: 0.00022015967174443218
loss: 0.00021998568105389778
loss: 0.00021973349721239675
loss: 0.00021914642420057162
loss: 0.00021809865738010154
loss: 0.00021599179702767128
loss: 0.00021120686068181916
loss: 0.0001982621135780452
loss: 0.00023946695365406297
loss: 0.0001803117133088088
loss: 0.00017624773867431305
loss: 0.00016430438347479786
loss: 0.00015623944295902262
loss: 0.0001473851951210903
loss: 0.0001422102628639802
loss: 0.00013776751360481155
loss: 0.00013597895853790877
test loss: 5.633607768933313e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.18269187  0.15963054  0.03377152 ... -0.17749491 -0.15592604
  -0.13353309]
 [-0.29622321 -0.40481007 -0.37368333 ... -0.1737087  -0.21492291
  -0.2548332 ]
 [-0.15517032 -0.30235827 -0.40394981 ...  0.03757119  0.06695553
   0.0972718 ]]
STEP:  3
loss: 0.00013421155454482333
loss: 0.00013291434545397257
loss: 0.00013054823547859026
loss: 0.0001275344567886074
loss: 0.00012061904247155948
loss: 0.00011522765472806572
loss: 0.00010684997431166869
loss: 0.00010107155570981078
loss: 9.238404757869267e-05
loss: 8.861205768888172e-05
loss: 8.630374012530932e-05
loss: 8.414840741772873e-05
loss: 8.332241382367926e-05
loss: 8.214883155798889e-05
loss: 8.09387712446314e-05
loss: 8.011423006571949e-05
loss: 7.938069293040263e-05
loss: 7.838191251369761e-05
loss: 7.706391553904897e-05
loss: 7.645376306767511e-05
test loss: 6.574030281429312e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.21022379  0.02682191 -0.09154607 ... -0.77604148 -0.78034999
  -0.78245198]
 [-0.32337616 -0.46403455 -0.41386217 ...  0.65586415  0.68499766
   0.71093813]
 [-0.16863976 -0.39168233 -0.47522664 ... -0.75976017 -0.75066122
  -0.74028187]]
STEP:  4
loss: 7.586241386601092e-05
loss: 7.548703142041314e-05
loss: 7.498876129099687e-05
loss: 7.426201613025272e-05
loss: 7.279530516821575e-05
loss: 7.003161672151665e-05
loss: 6.514260849733438e-05
loss: 6.601895388105068e-05
loss: 6.156900389299115e-05
loss: 6.341608514278405e-05
loss: 5.930304775042345e-05
loss: 5.8090421909817644e-05
loss: 5.7002685860544825e-05
loss: 5.3773896941247344e-05
loss: 5.134044427975851e-05
loss: 5.022085940789358e-05
loss: 4.909848560555873e-05
loss: 4.817951887492206e-05
loss: 4.635013989493386e-05
loss: 4.50082620666582e-05
test loss: 3.1923067479285545e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.16335746  0.06874585 -0.03223155 ...  0.55469243  0.55467597
   0.55466028]
 [-0.36633267 -0.4239394  -0.34540326 ...  0.55474306  0.55474567
   0.55474832]
 [-0.21098544 -0.34998489 -0.41028983 ...  0.55472798  0.55474099
   0.55475387]]
STEP:  5
loss: 4.467332351462808e-05
loss: 4.40649210974905e-05
loss: 4.371745541590228e-05
loss: 4.325408815369807e-05
loss: 4.290715675958247e-05
loss: 4.2511850703070474e-05
loss: 4.161985169657282e-05
loss: 4.1131040451080016e-05
loss: 4.1447515731721616e-05
loss: 4.00922867020912e-05
loss: 3.9743904724191217e-05
loss: 4.163049611341883e-05
loss: 3.857232331794622e-05
loss: 3.820729633059775e-05
loss: 3.803671759726546e-05
loss: 3.667563921848612e-05
loss: 3.629618202937339e-05
loss: 3.541916392546506e-05
loss: 3.4774457556827526e-05
loss: 3.4301317293771575e-05
test loss: 2.4277764029159636e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.1032356   0.07693868  0.00625354 ...  0.68010644  0.68010644
   0.68010643]
 [-0.4620094  -0.40484088 -0.27258736 ...  0.68010659  0.68010659
   0.68010659]
 [-0.29750092 -0.33818917 -0.35155455 ...  0.68010682  0.68010682
   0.68010682]]
STEP:  6
loss: 3.371128129239351e-05
loss: 3.31888440327658e-05
loss: 3.253615782688511e-05
loss: 3.137538698054506e-05
loss: 2.949813677740628e-05
loss: 2.6405810739406645e-05
loss: 2.4043236258579327e-05
loss: 2.2270992732521413e-05
loss: 2.183548156819507e-05
loss: 2.156407695149398e-05
loss: 2.5406902513997066e-05
loss: 2.0193674018565578e-05
loss: 1.9251014176948673e-05
loss: 1.829135638147131e-05
loss: 1.660010519689413e-05
loss: 1.5754961490188596e-05
loss: 1.5486258430111973e-05
loss: 1.5305282489770874e-05
loss: 1.518214648462598e-05
loss: 1.5045407374474415e-05
test loss: 1.3194365290750134e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.10332253  0.02898934 -0.03378741 ... -0.67463443 -0.68561987
  -0.69485072]
 [-0.4676417  -0.3964136  -0.27514825 ...  0.48668881  0.50682031
   0.52595601]
 [-0.30209451 -0.34914637 -0.36500178 ... -0.71236465 -0.70876977
  -0.70345966]]
STEP:  7
loss: 1.4813581306904072e-05
loss: 1.4519984097182781e-05
loss: 1.4243199022385936e-05
loss: 1.4034472737928193e-05
loss: 1.3900496839791858e-05
loss: 1.375469557547991e-05
loss: 1.354309631849058e-05
loss: 1.3357969920761302e-05
loss: 1.318411811506686e-05
loss: 1.3086233175140918e-05
loss: 1.2986080021683611e-05
loss: 1.2873058068367016e-05
loss: 1.2648655732498855e-05
loss: 1.2205648839370072e-05
loss: 1.132195592946899e-05
loss: 1.245408155386345e-05
loss: 1.077997888856083e-05
loss: 1.063788881773324e-05
loss: 1.029300406074176e-05
loss: 1.0154199077662426e-05
test loss: 1.0513321422141804e-05
-------PRED__y_pred.detach().numpy()__ [[ 0.10620428  0.03632722 -0.02675053 ...  0.90414362  0.88547683
   0.86527978]
 [-0.4328116  -0.38152053 -0.27561609 ... -0.89989257 -0.89371407
  -0.8855544 ]
 [-0.276826   -0.33317155 -0.35823396 ...  0.76179932  0.73435247
   0.70565873]]
STEP:  8
loss: 9.867652138042893e-06
loss: 9.626948469483404e-06
loss: 9.41628765096885e-06
loss: 9.358437571174016e-06
loss: 9.330063303484381e-06
loss: 9.29564432646251e-06
loss: 9.155045174470204e-06
loss: 9.20350018061985e-06
loss: 9.093245090929095e-06
loss: 9.084523407070213e-06
loss: 9.072151909232857e-06
loss: 9.066364647825172e-06
loss: 9.04314885272147e-06
loss: 8.991531748286158e-06
loss: 8.88485948163743e-06
loss: 8.70794820655079e-06
loss: 8.397499516624496e-06
loss: 1.6437414961363657e-05
loss: 8.104025385700121e-06
loss: 7.888617477764368e-06
test loss: 7.393961442984729e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.11013134  0.03257734 -0.02884208 ... -0.16515675 -0.2112607
  -0.25699406]
 [-0.41580385 -0.35663566 -0.26665076 ... -0.09213549 -0.04267488
   0.00717272]
 [-0.26262588 -0.31598217 -0.34964465 ... -0.47569933 -0.51640133
  -0.55577082]]
STEP:  9
loss: 7.710012540904234e-06
loss: 7.4208578522481455e-06
loss: 7.291517138205936e-06
loss: 7.100747094282829e-06
loss: 7.024199126781241e-06
loss: 6.931337392630756e-06
loss: 6.785789026718861e-06
loss: 6.710807013859007e-06
loss: 6.647354304349877e-06
loss: 6.597078657587873e-06
loss: 6.5744767560065944e-06
loss: 6.546677702093695e-06
loss: 6.528362849039483e-06
loss: 6.500096441000728e-06
loss: 6.434382019405426e-06
loss: 6.3431569724229285e-06
loss: 6.277398750561893e-06
loss: 6.268582443592637e-06
loss: 6.212180384306642e-06
loss: 6.164532757760673e-06
test loss: 7.0246768410220915e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.11029107  0.04236937 -0.02348168 ...  0.77469587  0.74097351
   0.70524827]
 [-0.41368564 -0.35978448 -0.27066927 ... -0.94241125 -0.9165504
  -0.88850964]
 [-0.26062561 -0.31380941 -0.34940212 ...  0.51904348  0.47316238
   0.42579254]]
STEP:  10
loss: 6.151697640814758e-06
loss: 6.13388319211361e-06
loss: 6.1224268913301415e-06
loss: 6.109998479485274e-06
loss: 6.099621641638523e-06
loss: 6.095183367987182e-06
loss: 6.092122820222793e-06
loss: 6.090185607155997e-06
loss: 6.08648665649104e-06
loss: 6.0779161368038046e-06
loss: 6.060284695185038e-06
loss: 6.027693456355621e-06
loss: 5.9565812212852615e-06
loss: 5.908992328632211e-06
loss: 6.759810320047516e-06
loss: 5.825673046355852e-06
loss: 5.802498463687013e-06
loss: 5.786618231227569e-06
loss: 5.797925819269848e-06
loss: 5.705331415638661e-06
test loss: 7.762673153220995e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.10018283  0.04687289 -0.01636349 ... -0.60707621 -0.556742
  -0.50497551]
 [-0.42698883 -0.35285076 -0.26513679 ...  1.13094112  1.12005982
   1.1080419 ]
 [-0.27290496 -0.30757507 -0.34257527 ... -0.16424614 -0.10595025
  -0.04720114]]
STEP:  11
loss: 5.6226667493385565e-06
loss: 5.422259501732586e-06
loss: 5.132058196479334e-06
loss: 5.2554932756968245e-06
loss: 4.9447931897691096e-06
loss: 6.281181147258573e-06
loss: 5.606471865648594e-06
loss: 4.9662821479137155e-06
loss: 4.868597019858042e-06
loss: 4.618785045408223e-06
loss: 4.518438611399406e-06
loss: 4.258400513214312e-06
loss: 4.193345800510585e-06
loss: 4.1713162505117846e-06
loss: 4.094541862728538e-06
loss: 4.1407843961215805e-06
loss: 4.041671242359129e-06
loss: 4.0194081295144854e-06
loss: 3.9992137300928494e-06
loss: 3.974537839279963e-06
test loss: 6.84928728507587e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.09440455  0.04058183 -0.01592741 ...  0.35601561  0.30622456
   0.25550188]
 [-0.42937432 -0.35053904 -0.27430222 ... -0.6216786  -0.57868363
  -0.53402519]
 [-0.27699438 -0.30876623 -0.34931283 ...  0.02264285 -0.03038436
  -0.08333762]]
STEP:  12
loss: 3.967644835902036e-06
loss: 3.9650978408084465e-06
loss: 3.961073909212721e-06
loss: 3.957157270685721e-06
loss: 3.95203845124801e-06
loss: 3.944219196421098e-06
loss: 3.928992034647228e-06
loss: 3.9018293725334765e-06
loss: 3.868468188699874e-06
loss: 3.838611277479197e-06
loss: 3.818363962082304e-06
loss: 3.808094900173841e-06
loss: 3.8020974357819847e-06
loss: 3.798886967639179e-06
loss: 3.79720786123706e-06
test loss: 6.8244705204362315e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.09529077  0.04231955 -0.01486418 ...  0.51320152  0.46636529
   0.41815822]
 [-0.43153633 -0.34426553 -0.2738083  ... -0.73471989 -0.69624192
  -0.65583281]
 [-0.27797721 -0.30416701 -0.3483923  ...  0.18665888  0.13410792
   0.08136658]]
STEP:  13
loss: 3.79720786123706e-06
test loss: 6.8244705204362315e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.09529077  0.04231955 -0.01486418 ...  0.51320152  0.46636529
   0.41815822]
 [-0.43153633 -0.34426553 -0.2738083  ... -0.73471989 -0.69624192
  -0.65583281]
 [-0.27797721 -0.30416701 -0.3483923  ...  0.18665888  0.13410792
   0.08136658]]
STEP:  14
loss: 3.79720786123706e-06
test loss: 6.8244705204362315e-06
-------PRED__y_pred.detach().numpy()__ [[ 0.09529077  0.04231955 -0.01486418 ...  0.51320152  0.46636529
   0.41815822]
 [-0.43153633 -0.34426553 -0.2738083  ... -0.73471989 -0.69624192
  -0.65583281]
 [-0.27797721 -0.30416701 -0.3483923  ...  0.18665888  0.13410792
   0.08136658]]
