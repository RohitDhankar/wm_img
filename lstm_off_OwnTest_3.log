----initial_data___
 [[ 0.10819513  0.05837414  0.00840725 -0.04158066 -0.09146464 -0.14112001
  -0.19042265 -0.23924933 -0.28747801 -0.33498815]
 [-0.38941834 -0.34289781 -0.29552021 -0.24740396 -0.19866933 -0.14943813
  -0.09983342 -0.04997917  0.          0.04997917]]
---input.shape----
 torch.Size([0, 9])
---target.shape----
 torch.Size([0, 9])
---test_input.shape----
 torch.Size([2, 9])
---test_target.shape----
 torch.Size([2, 9])
STEP:  0
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  1
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  2
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  3
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  4
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  5
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  6
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  7
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  8
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  9
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  10
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  11
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  12
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  13
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
STEP:  14
loss: nan
test loss: 0.05598205762533875
-------PRED__y_pred.detach().numpy()__ [[0.06396055 0.05808593 0.05520517 0.05393833 0.05347591 0.0533766
  0.0534168  0.05349267 0.05356197 0.05350854 0.05349533 0.05352921
  0.05358276 0.05363613 0.05368032 0.05371339 0.0537366  0.05375218
  0.05376232]
 [0.06417307 0.05833617 0.0553655  0.05396944 0.05339195 0.05321241
  0.05320928 0.05327285 0.05335304 0.05342899 0.053509   0.05358271
  0.05364281 0.05368778 0.05371949 0.05374091 0.05375494 0.05376393
  0.05376959]]
